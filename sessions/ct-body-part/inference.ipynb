{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/italati/AI-Deep-Learning-Lab-2022/blob/it-nb-0/sessions/ct-body-part/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKYuZWZ0gP9z"
      },
      "source": [
        "# Deep Learning for Automatic Labeling of Body CT Images\n",
        "\n",
        "### In this session, we will explore how we can utilize a pre-trained machine learning algorithm to predict the chest, abdomen, or pelvis slices in a body CT.\n",
        "\n",
        "*The pre-trained algorithm utilized in this notebook was developed by Ian Pan, MD. ai, and modified by Anouk Stein, MD.ai and Ross Filice MD, MedStar Georgetown University Hospital. The training code can be found [here](https://github.com/RSNA/AI-Deep-Learning-Lab-2022/blob/main/sessions/ct-body-part/train.ipynb).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acquire Image Data and Pre-trained Model\n",
        "\n",
        "We will clone the bodypart repository from github which includes the data we will use for this tutorial."
      ],
      "metadata": {
        "id": "QzKikyE7dV0V"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7RP28IXPGvG"
      },
      "source": [
        "!git clone https://github.com/rwfilice/bodypart.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Packages\n",
        "\n",
        "Google Colab comes preloaded with several packages that we can use but does not include everything. Pydicom is a package that will allow us to read, modify, and write DICOM data easily in python. We will install it here."
      ],
      "metadata": {
        "id": "94eKCmUEddqY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZPTc-wGP82S"
      },
      "source": [
        "!pip install pydicom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Python packages\n"
      ],
      "metadata": {
        "id": "-EfHihv51oVi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHp444ni3J7i"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras import activations\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "import numpy as np\n",
        "from keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
        "from keras import Model\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import re\n",
        "from scipy.ndimage.interpolation import zoom\n",
        "from keras.models import load_model\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import seaborn as sns\n",
        "import sys\n",
        "sys.path.insert(0,'/content/bodypart/')\n",
        "from saliency import *\n",
        "\n",
        "np.set_printoptions(suppress=True,threshold=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Convert Images\n",
        "\n",
        "Here we will create a list of the paths to our DICOM images which are stored in numpy arrays.\n",
        "\n",
        "We will use the glob function in the glob module to create a list with the pathname of all our test images using a specific pattern. "
      ],
      "metadata": {
        "id": "-pxCORRQiRGE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSK1Br4Gn9Ma"
      },
      "source": [
        "#Provide path to test images\n",
        "testPath = Path('bodypart/testnpy')\n",
        "testList = list(sorted(testPath.glob('**/*.npy'), key=lambda fn: int(re.search('-([0-9]*)', str(fn)).group(1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testList"
      ],
      "metadata": {
        "id": "RhTAYjV0f5PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a list of our test images, we need to define two functions. The first function will return the DICOM image array from the stored numpy file when provided with the path. The second function will convert a 512 x 512 pixel 16 bit DICOM image array into an 256 x 256 pixel 8 bit image array. We will use these functions to load and process our images before feeding them into our model."
      ],
      "metadata": {
        "id": "U8flshCztoYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dicom_and_uid(path_to_npy):\n",
        "    '''\n",
        "    Given a filepath, return the npy file and corresponding SOPInstanceUID. \n",
        "    '''\n",
        "    path_to_npy = str(path_to_npy)\n",
        "    dicom_file = np.load(path_to_npy)\n",
        "    uid = path_to_npy.split('/')[-1].replace('.npy', '')\n",
        "    return dicom_file, uid"
      ],
      "metadata": {
        "id": "2xmuw6vxewNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dicom_to_8bit(npy_file, width, level, imsize=(256.,256.), clip=True): \n",
        "    '''\n",
        "    Given a DICOM file, window specifications, and image size, \n",
        "    return the image as a Numpy array scaled to [0,255] of the specified size. \n",
        "    '''\n",
        "    array = npy_file.copy() \n",
        "    #array = array + int(dicom_file.RescaleIntercept) #we did this on preprocess\n",
        "    #array = array * int(dicom_file.RescaleSlope) #we did this on preprocess\n",
        "    array = np.clip(array, level - width / 2, level + width / 2)\n",
        "    # Rescale to [0, 255]\n",
        "    array -= np.min(array) \n",
        "    array /= np.max(array) \n",
        "    array *= 255.\n",
        "    array = array.astype('uint8')\n",
        "    \n",
        "    if clip:\n",
        "    # Sometimes there is dead space around the images -- let's get rid of that\n",
        "        nonzeros = np.nonzero(array) \n",
        "        x1 = np.min(nonzeros[0]) ; x2 = np.max(nonzeros[0])\n",
        "        y1 = np.min(nonzeros[1]) ; y2 = np.max(nonzeros[1])\n",
        "        array = array[x1:x2,y1:y2]\n",
        "\n",
        "    # Resize image if necessary\n",
        "    resize_x = float(imsize[0]) / array.shape[0] \n",
        "    resize_y = float(imsize[1]) / array.shape[1] \n",
        "    if resize_x != 1. or resize_y != 1.:\n",
        "        array = zoom(array, [resize_x, resize_y], order=1, prefilter=False)\n",
        "    return np.expand_dims(array, axis=-1)"
      ],
      "metadata": {
        "id": "wcT8LebXe3_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dicom_file, uid = get_dicom_and_uid('bodypart/testnpy/d2bb80e260ba5cd45aaaba9617f40f0d-74.npy')\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(dicom_file)\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(convert_dicom_to_8bit(dicom_file, 1000, 250).reshape((256,256)))"
      ],
      "metadata": {
        "id": "sv3xnTqreFak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load our stored model and trained weights\n",
        "\n",
        "In order to recreate our model to make inferences, we will load our stored model architecture and its pre-trained weights. The model is stored in JSON format and it's weights are stored in a H5 file."
      ],
      "metadata": {
        "id": "uVnwPBn1h5L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "json_file = open('bodypart/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)"
      ],
      "metadata": {
        "id": "qEgkbdT5ZDeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('bodypart/tcga-mguh-multilabel.h5')  #federated"
      ],
      "metadata": {
        "id": "tn-tuTwjZdNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict\n",
        "Now we will define a function that will allow us to load and process our test images, and feed them into our algorithm for a prediction."
      ],
      "metadata": {
        "id": "rM8CiP--t4ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference\n",
        "IMSIZE = 256\n",
        "WINDOW_LEVEL, WINDOW_WIDTH = 50, 500\n",
        "def predict(model, images, imsize):\n",
        "    '''\n",
        "    Small modifications to data generator to allow for prediction on test data.\n",
        "    '''\n",
        "    test_arrays = [] \n",
        " \n",
        "    test_probas = [] \n",
        "    test_uids   = []\n",
        "\n",
        "    #iterate through the testPath list and predict on each slice \n",
        "    for im in images: \n",
        "        dicom_file, uid = get_dicom_and_uid(im) \n",
        "        try:\n",
        "            array = convert_dicom_to_8bit(dicom_file, WINDOW_WIDTH, WINDOW_LEVEL, \n",
        "                                    imsize=(imsize,imsize))\n",
        "        except: \n",
        "            continue\n",
        "      \n",
        "        array = preprocess_input(array, mode='tf')\n",
        "        test_arrays.append(array) \n",
        "\n",
        "        test_probas.append(model.predict(np.expand_dims(array, axis=0)))\n",
        "        test_uids.append(uid)\n",
        "    return test_uids, test_arrays, test_probas\n",
        "                                    "
      ],
      "metadata": {
        "id": "vm7JM_pvZgXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we run the following cell, we use the function we defined above to predict and store our results in three separate lists. \n",
        "\n",
        "The uids list contains the unique identifier for each DICOM slice, the X list contains a 3D array of the image pixel data for each slice, and the y_prob list contains a 1D array containing the prediction for each slice."
      ],
      "metadata": {
        "id": "n1-kf2xf2BTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uids, X, y_prob = predict(model, testList, IMSIZE)"
      ],
      "metadata": {
        "id": "xOFRLIXGZgZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then store this information into a pandas dataframe, which is a tabular data structure."
      ],
      "metadata": {
        "id": "hieoAOhI8M5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_df = pd.DataFrame({'uid': uids, 'X': X, 'y_prob': y_prob})"
      ],
      "metadata": {
        "id": "ozQ0lLP6769r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Our Results\n",
        "\n",
        "Let's take a look at our pandas dataframe we created in the last cell."
      ],
      "metadata": {
        "id": "oyQQpccm9c_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_df"
      ],
      "metadata": {
        "id": "V0EXiQBu1_Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create three separate arrays from the \"y_prob\" column from the pandas dataframe. As seen above, the y_prob column contains an array for each image with three separate probabilities (one for each class: chest, abdomen, and pelvis). We will store each in a separate array called chest, abd, and pelv."
      ],
      "metadata": {
        "id": "FgzeG0Rx2MkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred_df.apply(lambda row: row['y_prob'], axis=1)"
      ],
      "metadata": {
        "id": "uOcBRO4_HYRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chest = np.stack(test_pred_df['y_prob'])[:,0][:,0]\n",
        "abd = np.stack(test_pred_df['y_prob'])[:,0][:,1]\n",
        "pelv = np.stack(test_pred_df['y_prob'])[:,0][:,2]\n",
        "\n",
        "chest, abd, pelv"
      ],
      "metadata": {
        "id": "Ko4vbZJQZgbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can plot these probability arrays on a graph."
      ],
      "metadata": {
        "id": "juhXPpjo_QBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(chest)\n",
        "plt.plot(abd)\n",
        "plt.plot(pelv)"
      ],
      "metadata": {
        "id": "TNwmIBL6Zl8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average over 5 slices to smooth out some of the spikes and see our 3 regions more clearly.\n",
        "\n"
      ],
      "metadata": {
        "id": "LvlFIKE0UKIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numaveslices = 5\n",
        "avepreds = []\n",
        "allpreds = np.stack(test_pred_df['y_prob'])[:,0]\n",
        "for idx,arr in enumerate(allpreds):\n",
        "    low = int(max(0,idx-(numaveslices-1)/2))\n",
        "    high = int(min(len(allpreds),idx+(numaveslices+1)/2))\n",
        "    avepreds.append(np.mean(allpreds[low:high],axis=0))\n",
        "\n",
        "chest = np.stack(avepreds)[:,0]\n",
        "abd = np.stack(avepreds)[:,1]\n",
        "pelv = np.stack(avepreds)[:,2]"
      ],
      "metadata": {
        "id": "etM-jKLbZl_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot averaged over 5 slices\n",
        "plt.plot(chest)\n",
        "plt.plot(abd)\n",
        "plt.plot(pelv)"
      ],
      "metadata": {
        "id": "LPUFCsMlZmBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot averaged over 5 slices, making it look fancy\n",
        "fig, ax1 = plt.subplots(figsize=(17,10))\n",
        "ax1.set_xlabel(\"Slice Number\", fontsize=20)\n",
        "ax1.set_ylabel(\"Confidence\", fontsize=20)\n",
        "plt.xticks([0,30,60,90,120,150,180,210],fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "ax1.axvline(30,color='gray',ymax=0.1)\n",
        "ax1.axvline(82,color='gray',ymax=0.1)\n",
        "ax1.axvline(120,color='gray',ymax=0.1)\n",
        "ax1.axvline(172,color='gray',ymax=0.1)\n",
        "ax1.axvline(195,color='gray',ymax=0.1)\n",
        "plt.plot(chest,linewidth=2,label=\"Chest\")\n",
        "plt.plot(abd,linewidth=2,label=\"Abdomen\")\n",
        "plt.plot(pelv,linewidth=2,label=\"Pelvis\")\n",
        "plt.legend(fontsize=16)"
      ],
      "metadata": {
        "id": "LdaijEtlZuqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets take a look at our images to see how our model is doing."
      ],
      "metadata": {
        "id": "qZS2idw1A7Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayImages(imgs,labels):\n",
        "    numimgs = len(imgs)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for idx,img in enumerate(imgs):\n",
        "        dicom_file, uid = get_dicom_and_uid(img)\n",
        "        img = convert_dicom_to_8bit(dicom_file, WINDOW_WIDTH, WINDOW_LEVEL, clip=False)\n",
        "        plt.subplot(\"1%i%i\" % (numimgs,idx+1))\n",
        "        plt.imshow(img[...,0],cmap='gray')\n",
        "        plt.title(labels[idx])\n",
        "        plt.axis('off')"
      ],
      "metadata": {
        "id": "feF4_ya0AjP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displayImages([testList[30],testList[82],testList[120],testList[172],testList[195]],[30,82,120,172,195])"
      ],
      "metadata": {
        "id": "WoTAdp5gZusw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saliency Maps / Bounding Boxes\n",
        "\n",
        "In this part of the tutorial, we will produce visual explanation heat maps that will help us understand how our algorithm makes decisions. This can help us visualize our model's mistakes and ultimately improve our model going forward.\n",
        "\n",
        "We will use the keras-vis library, which is a package that is already preloaded in Google Colab. It is a toolkit for visualizing and debugging trained keras neural network models. The documentation can be found [here.](https://pypi.org/project/tf-keras-vis/)\n",
        "\n",
        "At a high level, we will make use of gradients to determine pixel importance in the original image. There are many different algorithms that we may use to create saliency maps. For our example, we will use Grad-CAM, which is generally more applicable and requires less modifications to our network architecture.\n",
        "\n",
        "Intuitively, grad-CAM works by numerically computing the gradient of the output with respect to the input image. If certain pixels have a high gradient value, they have a strong contribution for prediction for that particular class. If they have a low gradient value, then they may not be as important for prediction. \n",
        "\n",
        "Therefore, we can use these gradients to highlight input regions that cause the most change in the output. Intuitively this should highlight salient image regions that most contribute towards the output."
      ],
      "metadata": {
        "id": "PT_TnZ5gfsAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell will provide us a summary of our model."
      ],
      "metadata": {
        "id": "DD4PT0TArOJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "print(len(model.layers))"
      ],
      "metadata": {
        "id": "x2QIlnUjffaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_idx = 156 #Index of last layer\n",
        "penultimate_layer=151 #Index of last convolutional layer\n",
        "#model.layers[-1].activation = activations.linear #Make the last layer linear (we don't need to, since the last layer is already linear (Dense))\n",
        "model.save('bodypart/grad_cm_5.hdf5')"
      ],
      "metadata": {
        "id": "uy6-6fa_ffqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gradcam = load_model('bodypart/grad_cm_5.hdf5')"
      ],
      "metadata": {
        "id": "da9dhsSffgDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are creating a python dictionary which allows us to define our labels according to their index (in our \"y_prob\" array that our model outputs). This will help us label our resulting image with the highest probability prediction."
      ],
      "metadata": {
        "id": "OMtq3KLIaWCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_CLASSES  = 3\n",
        "\n",
        "labels_dict1 = {'Chest':0, \n",
        "               'Abdomen':1,\n",
        "               'Pelvis':2}\n",
        "#mapping of labels to index\n",
        "\n",
        "labels_dict2 = {0:'Chest', \n",
        "               1:'Abdomen',\n",
        "               2:'Pelvis'}\n",
        "#mapping of index to labels\n",
        "N_CLASSES = len(labels_dict1)"
      ],
      "metadata": {
        "id": "pXab5dzhkTW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define our function that will predict on an input image, produce a saliency map of the prediction, and then subsequently overlay the saliency map onto our image. "
      ],
      "metadata": {
        "id": "qKNg8YoNbnx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayImages_saliency_maps(imgs,labels):\n",
        "    numimgs = len(imgs)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for idx,img in enumerate(imgs):\n",
        "        dicom_file, uid = get_dicom_and_uid(img)\n",
        "        img = convert_dicom_to_8bit(dicom_file, WINDOW_WIDTH, WINDOW_LEVEL, \n",
        "                            imsize=(IMSIZE,IMSIZE))\n",
        "        \n",
        "        array = preprocess_input(img, mode='tf')\n",
        "        probs = model.predict(np.expand_dims(array, axis=0)) #Do prediction on the given input image\n",
        "        index = np.argmax(probs) #Store the index of the class with the highest probability (0,1, or 2)\n",
        "        final_pred = labels_dict2[np.argmax(probs)] #Find the label of the prediction\n",
        "        saliency_map = visualize_cam(model_gradcam, layer_idx , filter_indices=index, seed_input=array,\n",
        "                              penultimate_layer_idx=penultimate_layer,\n",
        "                              backprop_modifier=None,grad_modifier=None) #Get the saliency maps of the prediction\n",
        "        #normalize output so it can be overlayed on image\n",
        "        saliency_map = (saliency_map-saliency_map.min())/(saliency_map.max()-saliency_map.min()) \n",
        "        heatmap = np.uint8(cm.jet(saliency_map[...,0])[..., :3] * 255) \n",
        "        original = np.uint8(cm.gray(img[...,0])[..., :3] * 255)\n",
        "\n",
        "        #Overlay the saliency map on the input image\n",
        "        overlaid_image = overlay(heatmap, original,0.2)\n",
        "    \n",
        "        plt.subplot(\"1%i%i\" % (numimgs,idx+1))\n",
        "        plt.imshow(overlaid_image)\n",
        "        plt.title(f'{final_pred}:{probs[0][index]}')\n",
        "        plt.axis('off')"
      ],
      "metadata": {
        "id": "qQmD4uLvfg3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displayImages_saliency_maps([testList[30],testList[82],testList[120],testList[172],testList[195]],[30,82,120,172,195])"
      ],
      "metadata": {
        "id": "nXD44J4QgH_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can utilize the saliency maps we produce to create bounding boxes around the region of interest with the highest prediction region. \n",
        "\n",
        "We use the opencv python package which is an open source computer vision/machine learning library. "
      ],
      "metadata": {
        "id": "Q7rscW-xje9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def displayImages_with_bbox(imgs,labels):\n",
        "    numimgs = len(imgs)\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for idx,img in enumerate(imgs):\n",
        "        dicom_file, uid = get_dicom_and_uid(img)\n",
        "        img = convert_dicom_to_8bit(dicom_file, WINDOW_WIDTH, WINDOW_LEVEL, \n",
        "                            imsize=(IMSIZE,IMSIZE))\n",
        "        \n",
        "        array = preprocess_input(img, mode='tf')\n",
        "        probs = model.predict(np.expand_dims(array, axis=0)) #Do prediction on the given input image\n",
        "        indice = np.argmax(probs)\n",
        "        final_pred = labels_dict2[np.argmax(probs)] #Find the label of the prediction\n",
        "        saliency_map = visualize_cam(model_gradcam, layer_idx , filter_indices=indice, seed_input=array,\n",
        "                              penultimate_layer_idx=penultimate_layer,\n",
        "                              backprop_modifier=None,grad_modifier=None)#Get the saliency maps of the prediction\n",
        "        gray = saliency_map[...,0] #Extract the red channel of the saliency map given highest prediction region\n",
        "        thresh = (gray>100).astype('uint8') #Threshold and extract the roi\n",
        "        cnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) #Find the bounding boxes of the heatmap roi\n",
        "        temp = img.copy()\n",
        "        temp = np.concatenate([temp,temp,temp],2)\n",
        "        \n",
        "        max_area = 0\n",
        "        i = 0\n",
        "        for index,c in enumerate(cnts):\n",
        "            if cv2.contourArea(c)>max_area:\n",
        "                max_area = cv2.contourArea(c)\n",
        "                i = index\n",
        "                \n",
        "        rect = cv2.boundingRect(cnts[i])\n",
        "        x,y,w,h = rect\n",
        "        temp = cv2.rectangle(temp,(x,y),(x+w,h+y),(255,0,0),6)      \n",
        "    \n",
        "        plt.subplot(\"1%i%i\" % (numimgs,idx+1))\n",
        "        plt.imshow(temp)\n",
        "        plt.title(f'{final_pred}:{probs[0][indice]}')\n",
        "        plt.axis('off')"
      ],
      "metadata": {
        "id": "DhZolxTogIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displayImages_with_bbox([testList[30],testList[82],testList[120],testList[172],testList[195]],[30,82,120,172,195])"
      ],
      "metadata": {
        "id": "zSXMbTQPgIj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration using T-SNE\n",
        "\n"
      ],
      "metadata": {
        "id": "K68O2MTeZh8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a statistical technique used to represent a high-dimensional dataset in two or three dimensions so that we can visualize separability. This technique was first described by Laurens van de Marten and Geoffrey Hinton in 2008 ([paper](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)).\n",
        "\n",
        "This technique can be useful prior to developing an algorithm to understand how easily our data can be separated into different classes. It pairs high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. It then defines a similar probability distribution on a low dimensional map. [Here is an example of a t-SNE of the MNIST data set](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding#/media/File:T-SNE_Embedding_of_MNIST.png)."
      ],
      "metadata": {
        "id": "VhGzdDm3uyu7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5ZH4V3BXOtr8"
      },
      "source": [
        "#Get path of train images\n",
        "trainPath = Path('bodypart/npy')\n",
        "trainList = list(sorted(trainPath.glob('**/*.npy'), key=lambda fn: int(re.search('-([0-9]*)', str(fn)).group(1))))\n",
        "len(trainList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purposes of this demonstration, we will use the same \"predict\" function we defined earlier to quickly process the 4,359 training slices into 8 bit dicom arrays. We will ignore the prediction results.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Nj-WsgEX2ByB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9lrJuRAQOtsA"
      },
      "source": [
        "#Stores the uids and their processed input images. We are using the same predict function we defined earlier, however we are only concerned with the processed 8 bit image arrays \n",
        "uids, X, k= predict(model, trainList, IMSIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's store our image data into a numpy array and take a look at the shape. We have 4,359 images, each with an image pixel array of 256 x 256 pixels, and one color channel. We will have to convert our 2D image into a 1D vector representation that T-SNE can work with. "
      ],
      "metadata": {
        "id": "zp0cEbFO2huV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.array(X)\n",
        "X_new.shape"
      ],
      "metadata": {
        "id": "ju7N9BdOygH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9NdpyYzOtsA"
      },
      "source": [
        "X_new = X_new.squeeze() #remove the color channel, since we are working in monochrome\n",
        "#Reshape all the processed input images to a 2 dimensional array\n",
        "X_new = X_new.reshape(X_new.shape[0],X_new.shape[1]*X_new.shape[2])\n",
        "X_new.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXEYTPSoOtsB"
      },
      "source": [
        "#Get the labels/ground truths of the input images, and store them in a pandas dataframe.\n",
        "df = pd.read_csv(\"bodypart/labels.csv\")\n",
        "Y = [] \n",
        "for i in range(len(trainList)):\n",
        "    label = labels_dict1[df[df['npyid']==str(trainList[i]).split('/')[-1].split('.')[0]]['label'].values[0]]\n",
        "    Y.append(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Randomly shuffle the order of input images\n",
        "np.random.seed(42)\n",
        "perm = np.random.permutation(X_new.shape[0])\n",
        "Y_new = np.array(Y)\n",
        "X_new.shape,Y_new.shape"
      ],
      "metadata": {
        "id": "NZ7pTY6lOusi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfJIHO1tOtsC"
      },
      "source": [
        "#x_new = X_new #If you want all points\n",
        "#y_new = Y_new #If you want all points\n",
        "x_new = X_new[perm][:3000] #Select random 3000 samples\n",
        "y_new = Y_new[perm][:3000] #Select random 3000 samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the t-SNE tool defined in the scikit learn package. We simply import the TSNE class, pass it our data (the images are in x_new, and the labels are in y_new), and then plot it."
      ],
      "metadata": {
        "id": "H-3DM4EZ0Gdh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUzCh9OsOtsC"
      },
      "source": [
        "#Plot t-sne \n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
        "z = tsne.fit_transform(x_new) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"y\"] = y_new\n",
        "df[\"comp-1\"] = z[:,0]\n",
        "df[\"comp-2\"] = z[:,1]\n",
        "df = df.replace({'y':labels_dict2})\n",
        "print(df['y'].value_counts())"
      ],
      "metadata": {
        "id": "rLxRD82SPNAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
        "                data=df).set(title=\"T-SNE\") "
      ],
      "metadata": {
        "id": "YPt4epiNPRXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "[1] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D. & Batra, D. (2017). Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.. ICCV (p./pp. 618-626), : IEEE Computer Society. ISBN: 978-1-5386-1032-9\n",
        "\n",
        "[2] van der Maaten, L. & Hinton, G. (2008). Visualizing Data using t-SNE . Journal of Machine Learning Research, 9, 2579--2605.\n",
        "\n"
      ],
      "metadata": {
        "id": "tJlMeNDfSImD"
      }
    }
  ]
}